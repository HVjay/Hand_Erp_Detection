{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot\n",
    "import scipy.fftpack \n",
    "from os import walk\n",
    "from sklearn.decomposition import PCA\n",
    "from packages import utils\n",
    "from scipy.signal import butter, lfilter,freqz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_into_channels(df):\n",
    "    df=df.drop(['Elements'], axis=1)\n",
    "    df=df.dropna()\n",
    "    dfs=np.split(df,[21,25],axis=1)\n",
    "    timeStamp=dfs[0]['TimeStamp']\n",
    "    dfs[1]=dfs[1].join(timeStamp)\n",
    "    df_channels=dfs[1]\n",
    "    df_channels=df_channels[[\"TimeStamp\",\"RAW_TP9\",\"RAW_AF7\",\"RAW_AF8\",\"RAW_TP10\"]]\n",
    "    raw_tp9=df_channels['RAW_TP9']\n",
    "    raw_tp10=df_channels['RAW_TP10']\n",
    "    raw_af7=df_channels['RAW_AF7']\n",
    "    raw_af8=df_channels['RAW_AF8']\n",
    "    raw_tp9=raw_tp9[1280:]\n",
    "    raw_tp10=raw_tp10[1280:]\n",
    "    raw_af7=raw_af7[1280:]\n",
    "    raw_af8=raw_af8[1280:]\n",
    "\n",
    "    return raw_tp9,raw_tp10,raw_af7,raw_af8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_channel(channel):\n",
    "    compressed=pd.Series()\n",
    "    lower_bound=0\n",
    "    for i in range(1,1201):\n",
    "        upper_bound=64*i\n",
    "        values=channel[lower_bound:upper_bound]\n",
    "#         print(\"NEXT\")\n",
    "#         print(\"LOWER {}\" .format(lower_bound))\n",
    "#         print(\"UPPER {}\" .format(upper_bound))\n",
    "        lower_bound=upper_bound\n",
    "        values=np.mean(values)\n",
    "        vals=pd.Series(values)\n",
    "        compressed=compressed.append(vals,ignore_index=True)\n",
    "    return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compressed_channel(raw_tp9,raw_tp10,raw_af7,raw_af8):\n",
    "    raw_tp9_compressed=compress_channel(raw_tp9)\n",
    "    raw_tp10_compressed=compress_channel(raw_tp10)\n",
    "    raw_af7_compressed=compress_channel(raw_af7)\n",
    "    raw_af8_compressed=compress_channel(raw_af8)\n",
    "    \n",
    "    return raw_tp9_compressed,raw_tp10_compressed,raw_af7_compressed,raw_af8_compressed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_labels(compressed_channel):\n",
    "    rest=[]\n",
    "    A=[]\n",
    "    B=[]\n",
    "\n",
    "    rest.append(compressed_channel[0:40])\n",
    "    A.append(compressed_channel[40:60])\n",
    "    rest.append(compressed_channel[60:100])\n",
    "    B.append(compressed_channel[100:120])\n",
    "\n",
    "    rest.append(compressed_channel[120:160])\n",
    "    A.append(compressed_channel[160:180])\n",
    "    rest.append(compressed_channel[180:220])\n",
    "    B.append(compressed_channel[220:240])\n",
    "\n",
    "    rest.append(compressed_channel[240:280])\n",
    "    A.append(compressed_channel[280:300])\n",
    "    rest.append(compressed_channel[300:340])\n",
    "    B.append(compressed_channel[340:360])\n",
    "\n",
    "    rest.append(compressed_channel[360:400])\n",
    "    A.append(compressed_channel[400:420])\n",
    "    rest.append(compressed_channel[420:460])\n",
    "    B.append(compressed_channel[460:480])\n",
    "\n",
    "    rest.append(compressed_channel[480:520])\n",
    "    A.append(compressed_channel[520:540])\n",
    "    rest.append(compressed_channel[540:580])\n",
    "    B.append(compressed_channel[580:600])\n",
    "\n",
    "    rest.append(compressed_channel[600:640])\n",
    "    A.append(compressed_channel[640:660])\n",
    "    rest.append(compressed_channel[660:700])\n",
    "    B.append(compressed_channel[700:720])\n",
    "\n",
    "    rest.append(compressed_channel[720:760])\n",
    "    A.append(compressed_channel[760:780])\n",
    "    rest.append(compressed_channel[780:820])\n",
    "    B.append(compressed_channel[820:840])\n",
    "\n",
    "    rest.append(compressed_channel[840:880])\n",
    "    A.append(compressed_channel[880:900])\n",
    "    rest.append(compressed_channel[900:940])\n",
    "    B.append(compressed_channel[940:960])\n",
    "\n",
    "    rest.append(compressed_channel[960:1000])\n",
    "    A.append(compressed_channel[1000:1020])\n",
    "    rest.append(compressed_channel[1020:1060])\n",
    "    B.append(compressed_channel[1060:1080])\n",
    "\n",
    "    rest.append(compressed_channel[1080:1120])\n",
    "    A.append(compressed_channel[1120:1140])\n",
    "    rest.append(compressed_channel[1140:1180])\n",
    "    B.append(compressed_channel[1180:1200])\n",
    "    \n",
    "    return A,B,rest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    tp9_A,tp9_B,tp9_rest=split_into_labels(raw_tp9_compressed)\n",
    "    tp10_A,tp10_B,tp10_rest=split_into_labels(raw_tp10_compressed)\n",
    "    af7_A,af7_B,af7_rest=split_into_labels(raw_af7_compressed)\n",
    "    af8_A,af8_B,af8_rest=split_into_labels(raw_af8_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeled_channel_to_np(labeled_channel):\n",
    "    labeled_channel_np=[]\n",
    "    for val,i in enumerate(labeled_channel):\n",
    "        labeled_channel_np.append(i.to_numpy())\n",
    "    return labeled_channel_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_single_row(tp9,tp10,af7,af8,group):\n",
    "    if group=='A':\n",
    "        label=[0,1,0]\n",
    "    elif group=='B':\n",
    "        label=[0,0,1]\n",
    "    else:\n",
    "        label=[1,0,0]\n",
    "    master=[]\n",
    "    for val,i in enumerate(tp9):\n",
    "        for index,j in enumerate(i):\n",
    "            single_row=[]\n",
    "            single_row.append(tp9[val][index])\n",
    "            single_row.append(tp10[val][index])\n",
    "            single_row.append(af7[val][index])\n",
    "            single_row.append(af8[val][index])\n",
    "            single_row.append(label)\n",
    "\n",
    "            master.append(single_row)\n",
    "    for i in range(0,3):\n",
    "        single_row=[]\n",
    "        single_row.append(tp9[val][index])\n",
    "        single_row.append(tp10[val][index])\n",
    "        single_row.append(af7[val][index])\n",
    "        single_row.append(af8[val][index])\n",
    "        single_row.append(label)\n",
    "        master.append(single_row)\n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "    # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "    # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "#         seq_x = sequences[i:end_ix, :-1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(labelled_X):\n",
    "    X_a_pca=[]\n",
    "    pca=PCA(n_components=4)\n",
    "    for val,i in enumerate(labelled_X):\n",
    "    #     if val>2:\n",
    "    #         break\n",
    "#         pca.fit(i)\n",
    "#         X_a_pca.append(pca.singular_values_)\n",
    "        X_a_pca.append(pca.fit_transform(i))\n",
    "    return X_a_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_dataset(df):\n",
    "    raw_tp9,raw_tp10,raw_af7,raw_af8=split_df_into_channels(df)\n",
    "    filtered_tp9=butter_lowpass_filter(raw_tp9,30,256,6)\n",
    "    filtered_tp10=butter_lowpass_filter(raw_tp10,30,256,6)\n",
    "    filtered_af7=butter_lowpass_filter(raw_af7,30,256,6)\n",
    "    filtered_af8=butter_lowpass_filter(raw_af8,30,256,6)\n",
    "    filtered_tp9_compressed,filtered_tp10_compressed,filtered_af7_compressed,filtered_af8_compressed=get_compressed_channel(\n",
    "                                                                                                        filtered_tp9,\n",
    "                                                                                                        filtered_tp10,\n",
    "                                                                                                        filtered_af7,\n",
    "                                                                                                        filtered_af8)\n",
    "    \n",
    "    tp9_A,tp9_B,tp9_rest=split_into_labels(filtered_tp9_compressed)\n",
    "    tp10_A,tp10_B,tp10_rest=split_into_labels(filtered_tp10_compressed)\n",
    "    af7_A,af7_B,af7_rest=split_into_labels(filtered_af7_compressed)\n",
    "    af8_A,af8_B,af8_rest=split_into_labels(filtered_af8_compressed)\n",
    "    \n",
    "    tp9_A_np=labeled_channel_to_np(tp9_A)\n",
    "    tp10_A_np=labeled_channel_to_np(tp10_A)\n",
    "    af7_A_np=labeled_channel_to_np(af7_A)\n",
    "    af8_A_np=labeled_channel_to_np(af8_A)\n",
    "\n",
    "    tp9_B_np=labeled_channel_to_np(tp9_B)\n",
    "    tp10_B_np=labeled_channel_to_np(tp10_B)\n",
    "    af7_B_np=labeled_channel_to_np(af7_B)\n",
    "    af8_B_np=labeled_channel_to_np(af8_B)\n",
    "\n",
    "    tp9_rest_np=labeled_channel_to_np(tp9_rest)\n",
    "    tp10_rest_np=labeled_channel_to_np(tp10_rest)\n",
    "    af7_rest_np=labeled_channel_to_np(af7_rest)\n",
    "    af8_rest_np=labeled_channel_to_np(af8_rest)\n",
    "    \n",
    "    master_A_np=np.array(convert_to_single_row(tp9_A_np,tp10_A_np,af7_A_np,af8_A_np,group='A'))\n",
    "    master_B_np=np.array(convert_to_single_row(tp9_B_np,tp10_B_np,af7_B_np,af8_B_np,group='B'))\n",
    "    master_rest_np=np.array(convert_to_single_row(tp9_rest_np,tp10_rest_np,af7_rest_np,af8_rest_np,group='rest'))\n",
    "    \n",
    "    \n",
    "   \n",
    "    X_a,Y_a=split_sequences(master_A_np,4)\n",
    "    X_b,Y_b=split_sequences(master_B_np,4)\n",
    "    X_rest,Y_rest=split_sequences(master_rest_np,4)\n",
    "    \n",
    "    X_a_pca=apply_pca(X_a)\n",
    "    X_b_pca=apply_pca(X_b)\n",
    "    X_rest_pca=apply_pca(X_rest)\n",
    "    X_a_pca_np=np.array(X_a_pca)\n",
    "    X_b_pca_np=np.array(X_b_pca)\n",
    "    X_rest_pca_np=np.array(X_rest_pca)\n",
    "    X_a=X_a_pca_np\n",
    "    X_b=X_b_pca_np\n",
    "    X_rest=X_rest_pca_np\n",
    "\n",
    "#     return master_A_np,Y_a,master_B_np,Y_b,master_rest_np,Y_rest\n",
    "    return X_a,Y_a,X_b,Y_b,X_rest,Y_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masters(X_a,Y_a,X_b,Y_b,X_rest,Y_rest):\n",
    "    rest_counter=0\n",
    "    A_counter=0\n",
    "    B_counter=0\n",
    "\n",
    "    X_master=[]\n",
    "    Y_master=[]\n",
    "    for i in range(1,6):\n",
    "        X_master.append(X_rest[(rest_counter)*20:(1+rest_counter)*20])\n",
    "        Y_master.append(Y_rest[(rest_counter)*20:(1+rest_counter)*20])\n",
    "        X_master.append(X_rest[(1+rest_counter)*20:(2+rest_counter)*20])\n",
    "        Y_master.append(Y_rest[(1+rest_counter)*20:(2+rest_counter)*20])\n",
    "\n",
    "        X_master.append(X_a[(A_counter)*20:(1+A_counter)*20])\n",
    "        Y_master.append(Y_a[(A_counter)*20:(1+A_counter)*20])\n",
    "\n",
    "        X_master.append(X_rest[(2+rest_counter)*20 : (3+rest_counter)*20])\n",
    "        Y_master.append(Y_rest[(2+rest_counter)*20:(3+rest_counter)*20])\n",
    "        X_master.append(X_rest[(3+rest_counter)*20 : (4+rest_counter)*20])\n",
    "        Y_master.append(Y_rest[(3+rest_counter)*20:(4+rest_counter)*20])\n",
    "\n",
    "        X_master.append(X_b[(B_counter)*20:(1+B_counter)*20])\n",
    "        Y_master.append(Y_b[(B_counter)*20:(1+B_counter)*20])\n",
    "\n",
    "        rest_counter=rest_counter+4\n",
    "        A_counter=A_counter+1\n",
    "        B_counter=B_counter+1\n",
    "        \n",
    "    X_master_np=np.array(X_master)\n",
    "    Y_master_np=np.array(Y_master)\n",
    "    return X_master_np,Y_master_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\n"
     ]
    }
   ],
   "source": [
    "files=[]\n",
    "path=r'C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files'\n",
    "for (dirpath, dirnames, filenames) in walk(path):\n",
    "    print(dirpath)\n",
    "    files.extend(filenames)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mindMonitor_2020-06-21--15-02-35.csv', 'mindMonitor_2020-06-22--11-39-14.csv', 'mindMonitor_2020-06-27--18-16-24.csv', 'mindMonitor_2020-06-27--18-31-15.csv', 'mindMonitor_2020-06-27--18-41-43.csv', 'mindMonitor_2020-06-27--18-56-16.csv', 'mindMonitor_2020-06-27--19-07-40.csv', 'mindMonitor_2020-06-27--19-14-36.csv', 'mindMonitor_2020-06-27--19-22-46.csv']\n"
     ]
    }
   ],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-21--15-02-35.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-22--11-39-14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-27--18-16-24.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-27--18-31-15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-27--18-41-43.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-27--18-56-16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-27--19-07-40.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-27--19-14-36.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\harri\\\\Desktop\\\\CorTech\\\\Hand_ErpDetection\\\\Recordings_2\\\\CSV_Files\\\\mindMonitor_2020-06-27--19-22-46.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "C:\\Users\\harri\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    }
   ],
   "source": [
    "X_master=[]\n",
    "Y_master=[]\n",
    "for file_names in files:\n",
    "    file_dir=path+r'\\\\'+file_names\n",
    "    print(file_dir)\n",
    "    df=pd.read_csv(file_dir)\n",
    "# master_A_np,Y_a,master_B_np,Y_b,master_rest_np,Y_rest=assemble_dataset(df)\n",
    "\n",
    "    X_a,Y_a,X_b,Y_b,X_rest,Y_rest=assemble_dataset(df)\n",
    "    X_all,Y_all=create_masters(X_a,Y_a,X_b,Y_b,X_rest,Y_rest)\n",
    "    X_master.append(X_all)\n",
    "    Y_master.append(Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 4, 4)\n",
      "(5400, 3)\n"
     ]
    }
   ],
   "source": [
    "X_master_np=np.array(X_master)\n",
    "Y_master_np=np.array(Y_master)\n",
    "X_final=np.reshape(X_master_np,((X_master_np.shape[0] *600),4,4))\n",
    "Y_final=np.reshape(Y_master_np,((Y_master_np.shape[0] *600),3))\n",
    "print(X_final.shape)\n",
    "print(Y_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble training, testing data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y_final, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(25,activation='relu',input_shape=(4,4)))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3672 samples, validate on 648 samples\n",
      "Epoch 1/20\n",
      "3672/3672 - 4s - loss: 1.1161 - accuracy: 0.4681 - val_loss: 2.6489 - val_accuracy: 0.6385\n",
      "Epoch 2/20\n",
      "3672/3672 - 0s - loss: 0.9160 - accuracy: 0.6699 - val_loss: 2.4568 - val_accuracy: 0.6588\n",
      "Epoch 3/20\n",
      "3672/3672 - 0s - loss: 0.8488 - accuracy: 0.6721 - val_loss: 2.3693 - val_accuracy: 0.6616\n",
      "Epoch 4/20\n",
      "3672/3672 - 0s - loss: 0.8187 - accuracy: 0.6721 - val_loss: 2.3361 - val_accuracy: 0.6633\n",
      "Epoch 5/20\n",
      "3672/3672 - 0s - loss: 0.8033 - accuracy: 0.6724 - val_loss: 2.3283 - val_accuracy: 0.6667\n",
      "Epoch 6/20\n",
      "3672/3672 - 0s - loss: 0.7965 - accuracy: 0.6729 - val_loss: 2.3105 - val_accuracy: 0.6661\n",
      "Epoch 7/20\n",
      "3672/3672 - 0s - loss: 0.7901 - accuracy: 0.6740 - val_loss: 2.3085 - val_accuracy: 0.6672\n",
      "Epoch 8/20\n",
      "3672/3672 - 0s - loss: 0.7870 - accuracy: 0.6743 - val_loss: 2.3129 - val_accuracy: 0.6672\n",
      "Epoch 9/20\n",
      "3672/3672 - 0s - loss: 0.7833 - accuracy: 0.6746 - val_loss: 2.3160 - val_accuracy: 0.6644\n",
      "Epoch 10/20\n",
      "3672/3672 - 0s - loss: 0.7798 - accuracy: 0.6746 - val_loss: 2.3228 - val_accuracy: 0.6678\n",
      "Epoch 11/20\n",
      "3672/3672 - 0s - loss: 0.7780 - accuracy: 0.6748 - val_loss: 2.3282 - val_accuracy: 0.6661\n",
      "Epoch 12/20\n",
      "3672/3672 - 0s - loss: 0.7747 - accuracy: 0.6767 - val_loss: 2.3487 - val_accuracy: 0.6644\n",
      "Epoch 13/20\n",
      "3672/3672 - 0s - loss: 0.7725 - accuracy: 0.6754 - val_loss: 2.3193 - val_accuracy: 0.6639\n",
      "Epoch 14/20\n",
      "3672/3672 - 0s - loss: 0.7692 - accuracy: 0.6767 - val_loss: 2.3038 - val_accuracy: 0.6616\n",
      "Epoch 15/20\n",
      "3672/3672 - 0s - loss: 0.7673 - accuracy: 0.6781 - val_loss: 2.3176 - val_accuracy: 0.6588\n",
      "Epoch 16/20\n",
      "3672/3672 - 0s - loss: 0.7646 - accuracy: 0.6784 - val_loss: 2.3265 - val_accuracy: 0.6650\n",
      "Epoch 17/20\n",
      "3672/3672 - 0s - loss: 0.7626 - accuracy: 0.6781 - val_loss: 2.3172 - val_accuracy: 0.6639\n",
      "Epoch 18/20\n",
      "3672/3672 - 0s - loss: 0.7605 - accuracy: 0.6806 - val_loss: 2.3193 - val_accuracy: 0.6610\n",
      "Epoch 19/20\n",
      "3672/3672 - 0s - loss: 0.7580 - accuracy: 0.6811 - val_loss: 2.3602 - val_accuracy: 0.6661\n",
      "Epoch 20/20\n",
      "3672/3672 - 0s - loss: 0.7569 - accuracy: 0.6789 - val_loss: 2.3405 - val_accuracy: 0.6684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d9b8f2d080>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,batch_size=60,epochs=20,verbose=2, validation_split=0.15,validation_steps=30,\n",
    "          use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080/1 - 0s\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict(X_test,verbose=2)\n",
    "test_predictions=np.argmax(predictions,axis=1)\n",
    "test_Y_pred=np.argmax(Y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.98      0.79       708\n",
      "           1       0.35      0.04      0.07       189\n",
      "           2       0.29      0.04      0.07       183\n",
      "\n",
      "    accuracy                           0.65      1080\n",
      "   macro avg       0.44      0.35      0.31      1080\n",
      "weighted avg       0.55      0.65      0.54      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_Y_pred,test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
